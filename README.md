# ğŸ§  Simple Local RAG â€” Retrieval-Augmented Generation with Local LLMs

This project is a complete implementation of a **fully local Retrieval-Augmented Generation (RAG)** system using **Jupyter Notebook**. It allows you to query your own documents (e.g., PDFs, text files) and get accurate, context-aware answers generated by an open-source **LLM**, without relying on any APIs or Docker.

---

## ğŸš€ Features

- ğŸ” **Document Chunking**: Splits large documents into manageable context windows.
- ğŸ§  **Semantic Search**: Uses sentence embeddings to retrieve the most relevant text chunks.
- ğŸ’¬ **LLM-based Answer Generation**: Augments user queries with retrieved content for accurate responses.
- ğŸ§° **Fully Local**: No API keys, no Docker â€” runs on your own machine.
- ğŸ“Š **Evaluation Ready**: Can be extended to include accuracy, relevance, and hallucination metrics.

---

## ğŸ›  Tech Stack

- Python 3.10+
- Jupyter Notebook
- HuggingFace Transformers
- SentenceTransformers
- FAISS (for vector search)
- Open-source LLMs (e.g., Mistral, LLaMA via HuggingFace)

---

## ğŸ“¦ Setup Instructions

### 1. Clone the Repo
git clone https://github.com/Anju-93905063/LLM.git
cd LLM
2. Create a Virtual Environment
python -m venv venv
venv\Scripts\activate  # On Windows
3. Install Dependencies
pip install -r requirements.txt
4. Launch Jupyter Notebook
jupyter notebook

ğŸ“Œ Example Use Case
"Query a 1,200-page PDF and get LLM-generated answers grounded in the document â€” all without internet."
