# 🧠 Simple Local RAG — Retrieval-Augmented Generation with Local LLMs

This project is a complete implementation of a **fully local Retrieval-Augmented Generation (RAG)** system using **Jupyter Notebook**. It allows you to query your own documents (e.g., PDFs, text files) and get accurate, context-aware answers generated by an open-source **LLM**, without relying on any APIs or Docker.

---

## 🚀 Features

- 🔍 **Document Chunking**: Splits large documents into manageable context windows.
- 🧠 **Semantic Search**: Uses sentence embeddings to retrieve the most relevant text chunks.
- 💬 **LLM-based Answer Generation**: Augments user queries with retrieved content for accurate responses.
- 🧰 **Fully Local**: No API keys, no Docker — runs on your own machine.
- 📊 **Evaluation Ready**: Can be extended to include accuracy, relevance, and hallucination metrics.

---

## 🛠 Tech Stack

- Python 3.10+
- Jupyter Notebook
- HuggingFace Transformers
- SentenceTransformers
- FAISS (for vector search)
- Open-source LLMs (e.g., Mistral, LLaMA via HuggingFace)

---

## 📦 Setup Instructions

### 1. Clone the Repo
git clone https://github.com/Anju-93905063/LLM.git
cd LLM
2. Create a Virtual Environment
python -m venv venv
venv\Scripts\activate  # On Windows
3. Install Dependencies
pip install -r requirements.txt
4. Launch Jupyter Notebook
jupyter notebook

📌 Example Use Case
"Query a 1,200-page PDF and get LLM-generated answers grounded in the document — all without internet."
